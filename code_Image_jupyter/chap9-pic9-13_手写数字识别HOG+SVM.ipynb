{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 图像处理系统的一般处理流程\n",
    "> * 图像采样、图像预处理（包括去噪、复原、校正等）、图像分割\n",
    "> * 生成训练数据集和测试数据集\n",
    "> * 特征提取\n",
    "> * 建立模型对像（分类、聚类、回归等）\n",
    "> * 训练模型\n",
    "> * 测试模型\n",
    "> * 模型评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000,)\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.externals import joblib\n",
    "from sklearn import datasets\n",
    "from skimage.feature import hog\n",
    "from sklearn.svm import LinearSVC\n",
    "import numpy as np\n",
    "import os,math,cv2,struct\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlinBackend.figure_format=\"retina\"\n",
    "plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus']=False #用来正常显示负\n",
    "\n",
    "#读取MNIST数据\n",
    "#path是数据文件夹的路径，labelfile是图像标注文件名,datafile是数据文件名\n",
    "def load_mnist(path,labelfile,datafile):            #读取数据函数\n",
    "    #Load MNIST data from path\n",
    "    labels_path = os.path.join(path, labelfile)\n",
    "    images_path = os.path.join(path, datafile)\n",
    "\n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II',lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, dtype=np.uint8)\n",
    "\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\",imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, dtype=np.uint8).reshape(len(labels), 784)\n",
    "    return images, labels\n",
    "\n",
    "#读取训练集    features是图片数据,labels是对应的标注\n",
    "features,labels = load_mnist(r\"../mnist\",'train-labels-idx1-ubyte', 'train-images-idx3-ubyte')\n",
    "print(features.shape,labels.shape)\n",
    "# print(type(features),type(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集行数: 6000, 列数: 784\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#显示训练数据\n",
    "features=features[0:6000,:]\n",
    "labels=labels[0:6000]\n",
    "print('训练集行数: %d, 列数: %d' % (features.shape[0], features.shape[1]))\n",
    "x=np.array(features[0,:])  #提取第一行数据\n",
    "x = x.reshape([28, 28])\n",
    "plt.figure(figsize=(12,4))\n",
    "# plt.subplots_adjust(top=0,bottom=-1)\n",
    "plt.imshow(x,cmap=\"Greys\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集行数: 10000, 列数: 784\n"
     ]
    }
   ],
   "source": [
    "# # 读取测试集\n",
    "testfeatures,testlabels = load_mnist(r\"../mnist\",'t10k-labels-idx1-ubyte', 't10k-images-idx3-ubyte')\n",
    "print('测试集行数: %d, 列数: %d' % (testfeatures.shape[0], testfeatures.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 36)\n",
      "(10000, 36)\n"
     ]
    }
   ],
   "source": [
    "#提取训练集HOG特征\n",
    "list_hog_fd = [] \n",
    "for feature in features:\n",
    "    fd = hog(feature.reshape((28, 28)),     # hog 特征\n",
    "             orientations=9, \n",
    "             pixels_per_cell=(14, 14), \n",
    "             cells_per_block=(1, 1), \n",
    "             visualize=False)\n",
    "    list_hog_fd.append(fd)\n",
    "hog_features = np.array(list_hog_fd, 'float64')  #训练集的HOG特征\n",
    "print(hog_features.shape)\n",
    "#提取测试集HOG特征\n",
    "list_hog_fd = [] \n",
    "for feature in testfeatures:\n",
    "    fd = hog(feature.reshape((28, 28)),     # hog 特征\n",
    "             orientations=9, \n",
    "             pixels_per_cell=(14, 14), \n",
    "             cells_per_block=(1, 1), \n",
    "             visualize=False)\n",
    "    list_hog_fd.append(fd)\n",
    "hog_testfeatures = np.array(list_hog_fd, 'float64')   #测试集的HOG\n",
    "print(hog_testfeatures.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN分类算法\n",
    "<img src=\"../img/knn.jpg\"/>\n",
    "## 使用KNN算法需要准备的参数\n",
    "\n",
    "> * 准备输入、输出数据，划分训练集和测试集\n",
    "> * K值\n",
    "> * 迭代次数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集第一个图像数字是： 7\n",
      "分类精度： 0.8372\n"
     ]
    }
   ],
   "source": [
    "# KNN分类器进行分类\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "k=5\n",
    "kNN_model = KNeighborsClassifier(n_neighbors=k)  #建立KNN分类模型\n",
    "kNN_model.fit(hog_features, labels)               #用训练集训练KNN模型\n",
    "testnumber = kNN_model.predict(hog_testfeatures)   #对测试集中的图片进行预测，输出为测试集中各图片所属的类别\n",
    "KNNscore = kNN_model.score(hog_testfeatures, testlabels) #输入测试集及标注，对测试结果进行评估，输出为分类精度\n",
    "print(\"测试集第一个图像数字是：\",testnumber[0])  #输出第一张测试图片所属类别\n",
    "print(\"分类精度：\",KNNscore)     #输出测试集的分类精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#不使用HOG，而是提取图像的LBP直方图特征\n",
    "g_mapping=[\n",
    "    0, 1, 2, 3, 4, 58, 5, 6, 7, 58, 58, 58, 8, 58, 9, 10, \n",
    "    11, 58, 58, 58, 58, 58, 58, 58, 12, 58, 58, 58, 13, 58, 14, 15, \n",
    "    16, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, \n",
    "    17, 58, 58, 58, 58, 58, 58, 58, 18, 58, 58, 58, 19, 58, 20, 21, \n",
    "    22, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, \n",
    "    58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, \n",
    "    23, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, \n",
    "    24, 58, 58, 58, 58, 58, 58, 58, 25, 58, 58, 58, 26, 58, 27, 28, \n",
    "    29, 30, 58, 31, 58, 58, 58, 32, 58, 58, 58, 58, 58, 58, 58, 33, \n",
    "    58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 34, \n",
    "    58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, \n",
    "    58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 35, \n",
    "    36, 37, 58, 38, 58, 58, 58, 39, 58, 58, 58, 58, 58, 58, 58, 40, \n",
    "    58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 41, \n",
    "    42, 43, 58, 44, 58, 58, 58, 45, 58, 58, 58, 58, 58, 58, 58, 46, \n",
    "    47, 48, 58, 49, 58, 58, 58, 50, 51, 52, 58, 53, 54, 55, 56, 57]\n",
    "\n",
    "def LBP(I, radius=2, count=8):       #得到图像的LBP特征\n",
    "    dh = np.round([radius*math.sin(i*2*math.pi/count) for i in range(count)])\n",
    "    dw = np.round([radius*math.cos(i*2*math.pi/count) for i in range(count)])\n",
    "    I=I.reshape(28, 28)\n",
    "    height,width = I.shape\n",
    "    lbp = np.zeros(I.shape, dtype = np.int)\n",
    "    I1 = np.pad(I, radius, 'edge')\n",
    "    for k in range(count):\n",
    "        h,w = int(radius+dh[k]), int(radius+dw[k])\n",
    "        lbp += ((I>I1[h:h+height, w:w+width])<<k)\n",
    "    return lbp\n",
    "\n",
    "def calLbpHistogram(lbp, hCount=7, wCount=5, maxLbpValue=255): #分块计算lbp直方图\n",
    "    height,width = lbp.shape\n",
    "    res = np.zeros((hCount*wCount, max(g_mapping)+1), dtype=np.float)\n",
    "    assert(maxLbpValue+1 == len(g_mapping))\n",
    "    \n",
    "    for h  in range(hCount):\n",
    "        for w in range(wCount):\n",
    "            blk = lbp[height*h//hCount:height*(h+1)//hCount, width*w//wCount:width*(w+1)//wCount]\n",
    "            hist1 = np.bincount(blk.ravel(), minlength=maxLbpValue)\n",
    "            hist = res[h*wCount+w,:]\n",
    "            for v,k in zip(hist1, g_mapping):\n",
    "                hist[k] += v\n",
    "            hist /= hist.sum()\n",
    "    feature=res.reshape(res.shape[0]*res.shape[1],1)\n",
    "    return res\n",
    "lbp_features = np.array([calLbpHistogram(LBP(d)).ravel() for d in features])\n",
    "lbp_testfeatures = np.array([calLbpHistogram(LBP(d)).ravel() for d in testfeatures])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集第一个图像数字是： 7\n",
      "HOG+SVM分类精度： 0.8555\n"
     ]
    }
   ],
   "source": [
    "#使用SVM支持向量机分类器进行分类\n",
    "from sklearn.svm import LinearSVC\n",
    "#建立并训练SVM模型\n",
    "svm_model = LinearSVC()                                # 建立SVM模型\n",
    "svm_model.fit(hog_features, labels)                    # 训练\n",
    "# joblib.dump(clf, \"digits_cls.pkl\", compress=3)   # 模型保存\n",
    "\n",
    "# #测试SVM模型\n",
    "# svm_model = joblib.load(\"digits_cls.pkl\") \n",
    "test_est = svm_model.predict(hog_testfeatures)   #测试单个图像，输出结果为图像所属类别\n",
    "SVMscore = svm_model.score(hog_testfeatures, testlabels)\n",
    "print(\"测试集第一个图像数字是：\",testlabels[0])\n",
    "print(\"HOG+SVM分类精度：\",SVMscore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------混淆矩阵-----------\n",
      "[[ 913    5    9    2    3    4    4    6    7   27]\n",
      " [   7 1079    5    0   13    0   12   11    3    5]\n",
      " [  13    0  935   35   23    2    3   10    7    4]\n",
      " [   6    0   76  830   18   21    3    8   18   30]\n",
      " [  19    6   19    8  836    7   43    7   16   21]\n",
      " [   4    4    5   34    7  752   17    4   43   22]\n",
      " [  43    7    3    1   25   28  831    0   14    6]\n",
      " [  11    7   59   12   14    1    0  863   14   47]\n",
      " [  10    4   18   33   25   19   37   14  794   20]\n",
      " [  82   11   13   58   31   16   14   33   29  722]]\n",
      "--------分类评估报告-----------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.93      0.87       980\n",
      "           1       0.96      0.95      0.96      1135\n",
      "           2       0.82      0.91      0.86      1032\n",
      "           3       0.82      0.82      0.82      1010\n",
      "           4       0.84      0.85      0.85       982\n",
      "           5       0.88      0.84      0.86       892\n",
      "           6       0.86      0.87      0.86       958\n",
      "           7       0.90      0.84      0.87      1028\n",
      "           8       0.84      0.82      0.83       974\n",
      "           9       0.80      0.72      0.75      1009\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.85      0.85     10000\n",
      "weighted avg       0.86      0.86      0.85     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "print(\"--------混淆矩阵-----------\")\n",
    "print(metrics.confusion_matrix(testlabels, test_est, labels=[0, 1,2,3,4,5,6,7,8,9]))  # 混淆矩阵\n",
    "print(\"--------分类评估报告-----------\")\n",
    "print(metrics.classification_report(testlabels, test_est))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(testlabels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LinearSVC in module sklearn.svm.classes:\n",
      "\n",
      "class LinearSVC(sklearn.base.BaseEstimator, sklearn.linear_model.base.LinearClassifierMixin, sklearn.linear_model.base.SparseCoefMixin)\n",
      " |  LinearSVC(penalty='l2', loss='squared_hinge', dual=True, tol=0.0001, C=1.0, multi_class='ovr', fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0, random_state=None, max_iter=1000)\n",
      " |  \n",
      " |  Linear Support Vector Classification.\n",
      " |  \n",
      " |  Similar to SVC with parameter kernel='linear', but implemented in terms of\n",
      " |  liblinear rather than libsvm, so it has more flexibility in the choice of\n",
      " |  penalties and loss functions and should scale better to large numbers of\n",
      " |  samples.\n",
      " |  \n",
      " |  This class supports both dense and sparse input and the multiclass support\n",
      " |  is handled according to a one-vs-the-rest scheme.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <svm_classification>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  penalty : string, 'l1' or 'l2' (default='l2')\n",
      " |      Specifies the norm used in the penalization. The 'l2'\n",
      " |      penalty is the standard used in SVC. The 'l1' leads to ``coef_``\n",
      " |      vectors that are sparse.\n",
      " |  \n",
      " |  loss : string, 'hinge' or 'squared_hinge' (default='squared_hinge')\n",
      " |      Specifies the loss function. 'hinge' is the standard SVM loss\n",
      " |      (used e.g. by the SVC class) while 'squared_hinge' is the\n",
      " |      square of the hinge loss.\n",
      " |  \n",
      " |  dual : bool, (default=True)\n",
      " |      Select the algorithm to either solve the dual or primal\n",
      " |      optimization problem. Prefer dual=False when n_samples > n_features.\n",
      " |  \n",
      " |  tol : float, optional (default=1e-4)\n",
      " |      Tolerance for stopping criteria.\n",
      " |  \n",
      " |  C : float, optional (default=1.0)\n",
      " |      Penalty parameter C of the error term.\n",
      " |  \n",
      " |  multi_class : string, 'ovr' or 'crammer_singer' (default='ovr')\n",
      " |      Determines the multi-class strategy if `y` contains more than\n",
      " |      two classes.\n",
      " |      ``\"ovr\"`` trains n_classes one-vs-rest classifiers, while\n",
      " |      ``\"crammer_singer\"`` optimizes a joint objective over all classes.\n",
      " |      While `crammer_singer` is interesting from a theoretical perspective\n",
      " |      as it is consistent, it is seldom used in practice as it rarely leads\n",
      " |      to better accuracy and is more expensive to compute.\n",
      " |      If ``\"crammer_singer\"`` is chosen, the options loss, penalty and dual\n",
      " |      will be ignored.\n",
      " |  \n",
      " |  fit_intercept : boolean, optional (default=True)\n",
      " |      Whether to calculate the intercept for this model. If set\n",
      " |      to false, no intercept will be used in calculations\n",
      " |      (i.e. data is expected to be already centered).\n",
      " |  \n",
      " |  intercept_scaling : float, optional (default=1)\n",
      " |      When self.fit_intercept is True, instance vector x becomes\n",
      " |      ``[x, self.intercept_scaling]``,\n",
      " |      i.e. a \"synthetic\" feature with constant value equals to\n",
      " |      intercept_scaling is appended to the instance vector.\n",
      " |      The intercept becomes intercept_scaling * synthetic feature weight\n",
      " |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      " |      as all other features.\n",
      " |      To lessen the effect of regularization on synthetic feature weight\n",
      " |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
      " |  \n",
      " |  class_weight : {dict, 'balanced'}, optional\n",
      " |      Set the parameter C of class i to ``class_weight[i]*C`` for\n",
      " |      SVC. If not given, all classes are supposed to have\n",
      " |      weight one.\n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      " |  \n",
      " |  verbose : int, (default=0)\n",
      " |      Enable verbose output. Note that this setting takes advantage of a\n",
      " |      per-process runtime setting in liblinear that, if enabled, may not work\n",
      " |      properly in a multithreaded context.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional (default=None)\n",
      " |      The seed of the pseudo random number generator to use when shuffling\n",
      " |      the data for the dual coordinate descent (if ``dual=True``). When\n",
      " |      ``dual=False`` the underlying implementation of :class:`LinearSVC`\n",
      " |      is not random and ``random_state`` has no effect on the results. If\n",
      " |      int, random_state is the seed used by the random number generator; If\n",
      " |      RandomState instance, random_state is the random number generator; If\n",
      " |      None, the random number generator is the RandomState instance used by\n",
      " |      `np.random`.\n",
      " |  \n",
      " |  max_iter : int, (default=1000)\n",
      " |      The maximum number of iterations to be run.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  coef_ : array, shape = [n_features] if n_classes == 2 else [n_classes, n_features]\n",
      " |      Weights assigned to the features (coefficients in the primal\n",
      " |      problem). This is only available in the case of a linear kernel.\n",
      " |  \n",
      " |      ``coef_`` is a readonly property derived from ``raw_coef_`` that\n",
      " |      follows the internal memory layout of liblinear.\n",
      " |  \n",
      " |  intercept_ : array, shape = [1] if n_classes == 2 else [n_classes]\n",
      " |      Constants in decision function.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.svm import LinearSVC\n",
      " |  >>> from sklearn.datasets import make_classification\n",
      " |  >>> X, y = make_classification(n_features=4, random_state=0)\n",
      " |  >>> clf = LinearSVC(random_state=0, tol=1e-5)\n",
      " |  >>> clf.fit(X, y)  # doctest: +NORMALIZE_WHITESPACE\n",
      " |  LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      " |       intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      " |       multi_class='ovr', penalty='l2', random_state=0, tol=1e-05, verbose=0)\n",
      " |  >>> print(clf.coef_)\n",
      " |  [[0.085... 0.394... 0.498... 0.375...]]\n",
      " |  >>> print(clf.intercept_)\n",
      " |  [0.284...]\n",
      " |  >>> print(clf.predict([[0, 0, 0, 0]]))\n",
      " |  [1]\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The underlying C implementation uses a random number generator to\n",
      " |  select features when fitting the model. It is thus not uncommon\n",
      " |  to have slightly different results for the same input data. If\n",
      " |  that happens, try with a smaller ``tol`` parameter.\n",
      " |  \n",
      " |  The underlying implementation, liblinear, uses a sparse internal\n",
      " |  representation for the data that will incur a memory copy.\n",
      " |  \n",
      " |  Predict output may not match that of standalone liblinear in certain\n",
      " |  cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
      " |  in the narrative documentation.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  `LIBLINEAR: A Library for Large Linear Classification\n",
      " |  <https://www.csie.ntu.edu.tw/~cjlin/liblinear/>`__\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  SVC\n",
      " |      Implementation of Support Vector Machine classifier using libsvm:\n",
      " |      the kernel can be non-linear but its SMO algorithm does not\n",
      " |      scale to large number of samples as LinearSVC does.\n",
      " |  \n",
      " |      Furthermore SVC multi-class mode is implemented using one\n",
      " |      vs one scheme while LinearSVC uses one vs the rest. It is\n",
      " |      possible to implement one vs the rest with SVC by using the\n",
      " |      :class:`sklearn.multiclass.OneVsRestClassifier` wrapper.\n",
      " |  \n",
      " |      Finally SVC can fit dense data without memory copy if the input\n",
      " |      is C-contiguous. Sparse data will still incur memory copy though.\n",
      " |  \n",
      " |  sklearn.linear_model.SGDClassifier\n",
      " |      SGDClassifier can optimize the same cost function as LinearSVC\n",
      " |      by adjusting the penalty and loss parameters. In addition it requires\n",
      " |      less memory, allows incremental (online) learning, and implements\n",
      " |      various loss functions and regularization regimes.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LinearSVC\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.linear_model.base.LinearClassifierMixin\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      sklearn.linear_model.base.SparseCoefMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, penalty='l2', loss='squared_hinge', dual=True, tol=0.0001, C=1.0, multi_class='ovr', fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0, random_state=None, max_iter=1000)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit the model according to the given training data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      " |          Training vector, where n_samples in the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like, shape = [n_samples]\n",
      " |          Target vector relative to X\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Array of weights that are assigned to individual\n",
      " |          samples. If not provided,\n",
      " |          then each sample is given unit weight.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model.base.LinearClassifierMixin:\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Predict confidence scores for samples.\n",
      " |      \n",
      " |      The confidence score for a sample is the signed distance of that\n",
      " |      sample to the hyperplane.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      " |          Confidence scores per (sample, class) combination. In the binary\n",
      " |          case, confidence score for self.classes_[1] where >0 means this\n",
      " |          class would be predicted.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class labels for samples in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape [n_samples]\n",
      " |          Predicted class label per sample.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model.base.SparseCoefMixin:\n",
      " |  \n",
      " |  densify(self)\n",
      " |      Convert coefficient matrix to dense array format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      " |      default format of ``coef_`` and is required for fitting, so calling\n",
      " |      this method is only required on models that have previously been\n",
      " |      sparsified; otherwise, it is a no-op.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator\n",
      " |  \n",
      " |  sparsify(self)\n",
      " |      Convert coefficient matrix to sparse format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      " |      L1-regularized models can be much more memory- and storage-efficient\n",
      " |      than the usual numpy.ndarray representation.\n",
      " |      \n",
      " |      The ``intercept_`` member is not converted.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      " |      this may actually *increase* memory usage, so use this method with\n",
      " |      care. A rule of thumb is that the number of zero elements, which can\n",
      " |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      " |      to provide significant benefits.\n",
      " |      \n",
      " |      After calling this method, further fitting with the partial_fit\n",
      " |      method (if any) will not work until you call densify.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "help(LinearSVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zy\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一个图像数字是： 7\n",
      "分类精度： 0.819\n"
     ]
    }
   ],
   "source": [
    "#随机森林分类\n",
    "from sklearn.ensemble import RandomForestClassifier  \n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(hog_features, labels) \n",
    "testnumber = rf_model.predict(hog_testfeatures)   #测试单个图像，输出结果为图像所属类别\n",
    "rf_score = rf_model.score(hog_testfeatures, testlabels)\n",
    "print(\"第一个图像数字是：\",testnumber[0])\n",
    "print(\"分类精度：\",rf_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zy\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\zy\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一个图像数字是： 7\n",
      "分类精度： 0.853\n"
     ]
    }
   ],
   "source": [
    "# #逻辑回归分类\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(hog_features, labels) \n",
    "testnumber = lr_model.predict(hog_testfeatures)   #测试单个图像，输出结果为图像所属类别\n",
    "lr_score = lr_model.score(hog_testfeatures, testlabels)\n",
    "print(\"第一个图像数字是：\",testnumber[0])\n",
    "print(\"分类精度：\",lr_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''练习：实现HOG+KNN，HOG+SVM,HOG+随机森林分类，HOG+逻辑回归\n",
    "以及LBP+KNN，LBP+SVM,LBP+随机森林分类，LBP+逻辑回归的手写数字识别，比较他们的识别精度\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function adaptiveThreshold:\n",
      "\n",
      "adaptiveThreshold(...)\n",
      "    adaptiveThreshold(src, maxValue, adaptiveMethod, thresholdType, blockSize, C[, dst]) -> dst\n",
      "    .   @brief Applies an adaptive threshold to an array.\n",
      "    .   \n",
      "    .   The function transforms a grayscale image to a binary image according to the formulae:\n",
      "    .   -   **THRESH_BINARY**\n",
      "    .   \\f[dst(x,y) =  \\fork{\\texttt{maxValue}}{if \\(src(x,y) > T(x,y)\\)}{0}{otherwise}\\f]\n",
      "    .   -   **THRESH_BINARY_INV**\n",
      "    .   \\f[dst(x,y) =  \\fork{0}{if \\(src(x,y) > T(x,y)\\)}{\\texttt{maxValue}}{otherwise}\\f]\n",
      "    .   where \\f$T(x,y)\\f$ is a threshold calculated individually for each pixel (see adaptiveMethod parameter).\n",
      "    .   \n",
      "    .   The function can process the image in-place.\n",
      "    .   \n",
      "    .   @param src Source 8-bit single-channel image.\n",
      "    .   @param dst Destination image of the same size and the same type as src.\n",
      "    .   @param maxValue Non-zero value assigned to the pixels for which the condition is satisfied\n",
      "    .   @param adaptiveMethod Adaptive thresholding algorithm to use, see #AdaptiveThresholdTypes.\n",
      "    .   The #BORDER_REPLICATE | #BORDER_ISOLATED is used to process boundaries.\n",
      "    .   @param thresholdType Thresholding type that must be either #THRESH_BINARY or #THRESH_BINARY_INV,\n",
      "    .   see #ThresholdTypes.\n",
      "    .   @param blockSize Size of a pixel neighborhood that is used to calculate a threshold value for the\n",
      "    .   pixel: 3, 5, 7, and so on.\n",
      "    .   @param C Constant subtracted from the mean or weighted mean (see the details below). Normally, it\n",
      "    .   is positive but may be zero or negative as well.\n",
      "    .   \n",
      "    .   @sa  threshold, blur, GaussianBlur\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help( cv2.adaptiveThreshold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
